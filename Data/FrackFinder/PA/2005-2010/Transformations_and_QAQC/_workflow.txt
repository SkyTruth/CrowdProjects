# ====================== #
# == General Workflow == #
# ====================== #

1) Download all task.json and task_run.json for each application
2) Convert each application's JSON files into a single shapefile with one point per task and aggregated attributes from the task_run.json file
    - Attributes include number of total responses, number of times a given selection was chosen, crowd agreement level, etc.
3) Randomly sample shapefiles from step 2 - sample 100 sites from public applications and 50 from internal
4) Examine sampling results to determine an agreement tolerance in order to determine which tasks need to be re-examined
5) Extract tasks to be re-examined and load into a new internally accessible application - complete tasks
6) Stitch tasks from all applications into a single final dataset with one row per task
    - Columns contain information like final answer, where the task was completed, and attributes from step 2 for each application
7) Extract tasks that were confidently classified as fracking and digitize






== Tadpole ==
Manually sample 100 random sites for each year to make sure the crowd’s response is sane.  Sites were compared in QGIS by loading the appropriate imagery and examining each task individually to determine the site’s classification

No queries needed - public crowd completed all tasks

2005 - 6 disagreements
2008 - 4 disagreements
2010 - 2 disagreements



== MooreFrog ==
Sample 100 random sites and test for the following for omitted ponds, which were ponds within the area of interest that were not clicked on.

No queries needed - public crowd completed all tasks

2005 - 18 omitted ponds - None appear to be fracking related
2008 - 19 omitted ponds - None appear to be fracking related
2010 - 6 omitted ponds - 2 appear to be fracking related



== DartFrog ==
Verify crowd’s pond classification by manually examining each sample site.  Samples taken across all apps across all years.

Public sampling query: “p_crd_a” >= 80 AND “n_tot_res” >= 10
Internal sampling query: “p_crd_a” >= 66

2005 First Internal - Sampled 50 - disagreed with 2 pond classifications
2008 First Internal - Sampled 50 - disagreed with 5 pond classifications
2010 First Internal - Sampled 50 - disagreed with 4 pond classifications

2005 Final Internal - Sampled 50 - disagreed with 0 pond classifications
2008 Final Internal - Sampled 50 - disagreed with 0 pond classifications
2010 Final Internal - NO SAMPLE - All DartFrog 2010 tasks were completed by the public or the First Internal app

2005 Public - NO SAMPLE - All DartFrog 2005 tasks were completed in the internal applications
2008 Public - Sampled 99 - disagreed with 18 pond classifications (unsure why 99 sites were sampled and not 100)
2010 Public - Sampled 100 - disagreed with 10 pond classifications

2005 Sweeper Internal - Sampled all 6 sites - disagreed with 0 pond classifications
2008 Sweeper Internal - Sampled 50 - disagreed with 6 pond classifications
2010 Sweeper Internal - Sampled 50 - disagreed with 6 pond classifications



